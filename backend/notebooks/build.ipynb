{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adc94cc",
   "metadata": {},
   "source": [
    "## Reading in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb57c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from docx import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c96be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../app/data/\"\n",
    "\n",
    "doc = Document(data_path + \"bq_questions.docx\")\n",
    "\n",
    "full_text = \"\\n\".join([p.text for p in doc.paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6409a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Question#  Tell me about yourself: #Question#\\nMy name is Daoming Liu. I'm a data science graduate with a focus on natural language processing and applied machine learning, recently graduated from UBC’s MDS in Computational Linguistics. At the Digital Lab for BC Children’s Hospital, I built a Retrieval-Augmented Generation (RAG) chatbot to help clinicians access health resources more efficiently, reducing search time and improving usability in internal workflows. At Adauris, a TTS startup, I worked on backend development, designing API endpoints that enabled real-time LLM service integration, supporting their MVP launch, and client demos. \\n\\n#Question# Tell me about a time when you were asked to do something you had never done before. How did you react? What did you learn? #Question#\\nAt Adauris, I was collaborating on redesigning the backend structure for a new product feature. One of the endpoints I was assigned to update was written in TypeScript, which was a language I had never used before. At first, I was overwhelmed by the unfamiliar syntax and structure. Instead of diving in blindly, I spent the weekend learning the fundamentals of TypeScript and practicing by rewriting small components with the help of AI tools. This gave me the confidence to make meaningful updates to the endpoint. In the end, I completed the upgrade successfully, helping our team deliver the feature on time. I learned that grounding myself in the basics and quickly putting in hands-on work is a reliable strategy when facing new technical challenges.\\n\\n\\n#Question#  Recall a time when you were assigned a task outside of your job description. How did you handle the situation? What was the outcome? #Question#\\nAt Adauris, I was collaborating on redesigning the backend structure for a new product feature. One of the endpoints I was assigned to update was written in TypeScript, which of outside of my domain knowledge as a Python developer. When I first looked at the code, I was overwhelmed by the unfamiliar syntax and structure. My strategy of learning and tackling unfamiliar challenges was always to learn by doing. Therefore, after spending a weekend learning the fundamentals of TypeScript, I jumped right into the script and started practicing by rewriting small components with the help of AI tools. In the end, I completed the upgrade successfully, helping our team deliver the feature on time.\\n\\n\\n#Question#  Tell me about the biggest change you’ve had to deal with. How did you adapt to that change? #Question#\\nThe biggest change I’ve had to deal with was moving from China to the U.S. alone at age 14 to attend high school. I didn’t speak English fluently, and I was away from my family for the first time. In the first few months, I felt isolated – I avoided group activities and found it difficult to connect with others. Eventually, I realized that isolating myself, waiting for others to approach me, wouldn’t work,\\n\\n#Question#  Tell me about a time when you had to adjust to a colleague’s working style in order to complete a project or achieve your outcomes. #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked with three classmates to develop a RAG chatbot. There was one teammate in particular that I had to work very closely with, as my module depended on his implementation of his part. However, he had a last-minute working style and a busy schedule, and he often completed tasks close to deadlines. This conflicted with my need to iterate early.\\nTo adapt, I scheduled weekly alignment meetings early in each cycle. I prioritized completing the weekly goal on time over maintaining a strict “equal distribution of work”. Therefore, to keep us on track, I encouraged realistic discussion and planning based on his availability and took on some of his foundational tasks to prevent bottlenecks.\\nAs a result, we consistently met project milestones and delivered a functional prototype on time. This taught me the importance of flexibility and proactive communication when collaborating with colleagues with different working styles.\\n\\n#Question#  Give an example of when you had to work with someone who was difficult to get along with. How did you handle interactions with that person? #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked with three teammates to develop a RAG chatbot application. There was one teammate in particular that I had to work very closely with, as my module depended on his implementation in his part. However, this teammate had a very different working style from mine, in that he often starts his work right close to deadlines. At first, this caused tension because I couldn’t move forward without his progress.\\nTo manage this, I focused on open and respectful communication. I scheduled alignment meetings with him early in each cycle, where we discussed timelines and dependencies honestly. I prioritized delivering the project successfully over splitting the workload equally. Therefore, to reduce pressure on him and keep us on track, I often offered to help with some of his tasks.\\nAs a result, we avoided bottlenecks and consistently met our milestones. I learned that staying calm, proactive, and solution-focused is key when working with colleagues who have different approaches.\\n\\n#Question#  Tell me about a time when you were communicating with someone and they did not understand you. What did you do? #Question#\\nIn one of my courses at UBC, I worked with three teammates on an end-to-end project. One teammate was responsible for writing API endpoints, but I noticed, during a meeting, that she nodded and agreed to even plans that were quite vague and never asked any questions. I sensed she didn’t fully understand the task but might have been hesitant to ask questions.\\nTherefore, after the meeting, I reached out to her privately and asked about her plan and approach. As I expected, she couldn’t provide much detail and was confused about our entire structure in general. By posing questions, I showed her aspects that she would need to think about when working on her parts, and re-explained our plan step by step to her and answered her questions.\\nAs a result, she gained clarity and successfully delivered her module on time, fully aligned with the team’s expectations.\\n\\n#Question#  Tell me about one of your favorite experiences working with a team and the contributions you made. #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked with three teammates to develop a RAG chatbot application. \\n\\n\\n#Question#  Describe the best partner or supervisor you’ve worked with. What part of their management style appealed to you? #Question#\\nThe best supervisor I’ve worked with was my research mentor at UBC during a project on sign language machine translation. At times, I felt lost, such as when I struggled to connect our data preprocessing module with a large model and started doubting if our approach was even feasible. When I reached out, he first reassured me that the overall direction was sound based on his experience. Then, instead of giving me direct instructions, he pointed me to resources and potential methods I could explore.\\n\\nAfter reviewing those resources, I was able to resolve the issue and integrate the modules successfully, which moved our project forward. His management style appealed to me because he combined a clear vision with a supportive, hands-off approach. It not only kept the project on track but also encouraged my personal growth by training me to think independently and find solutions myself.\\n\\n#Question# Can you share an experience where a project dramatically shifted direction at the last minute? What did you do? #Question#\\nAt Adauris, I was developing a text-to-podcast feature for two weeks when, near the end of the cycle, the marketing team decided to change the podcast style and format based on new customer feedback. This shift was non-trivial and could take several more days to implement.\\nTo respond effectively, I first scheduled a meeting with the marketing lead to clarify their revised expectations. With limited time, I wanted to ensure we were fully aligned before starting any rework. Once the direction was confirmed, I met with my tech team to review our existing implementation and identify parts we could reuse or adjust to avoid redundant work. Together, we created a streamlined plan for the changes.\\nAs a result, we minimized unnecessary rework and successfully delivered the updated feature by the end of the cycle. This experience reinforced for me how critical clear communication and alignment are when managing last-minute changes.\\n\\n\\n#Question# Tell me about the last time something significant didn’t go according to plan at work. What was your role? What was the outcome? #Question#\\nDuring my Capstone project at BC Children’s Hospital, our original plan was to fine-tune a chatbot to answer patient questions. However, we soon realized that the data provided was insufficient and, combined with the tight timeline, model fine-tuning wasn’t feasible.\\nI acted as the bridge between my team and the stakeholders. After multiple internal discussions with my teammates, I communicated the challenge honestly and clearly to the stakeholders and presented alternative solutions that could align with the business goal. Eventually, we developed a simple demo chatbot using large language model APIs, along with an evaluation pipeline that allows stakeholders to test different models when more data becomes available.\\nWhile it wasn’t the solution we initially envisioned, the stakeholders appreciated our adaptability and the deliverables as a foundation for their future work.\\n\\n\\n#Question# Describe a situation where you needed to persuade someone to see things your way. What steps did you take? What were the results? #Question#\\nIn my Capstone project at BC Children’s Hospital, our initial task was to fine-tune a language model for a chatbot. However, due to limited data and a tight timeline, we realized this wasn’t feasible. After internal discussions, my team agreed that the most valuable deliverable could be a modular evaluation pipeline, allowing stakeholders to test different models in the future when more data becomes available.\\nTo persuade the stakeholders to accept this new direction, we took three steps. First, we built a simple chatbot demo using sample data to show we understood their original vision. Next, we outlined the data limitations and the technical challenges of fine-tuning. Finally, we proposed our alternative plans, highlighting their long-term value and feasibility.\\nAs a result, we gained their support for the new direction and successfully delivered a comprehensive evaluation pipeline, which the stakeholders appreciated as a practical foundation for future development.\\n\\n#Question#  Tell me about a time when you led by example. What did you do and how did others react? #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked with three classmates to develop a RAG chatbot. There was one teammate in particular that I had to work very closely with, as my module depended on his implementation of his part. However, he had a last-minute working style and a busy schedule, and he often completed tasks close to deadlines. This conflicted with my need to iterate early.\\nTo lead by example, I made it a priority to complete my tasks ahead of schedule and share updates proactively. I also scheduled weekly alignment meetings early in each cycle and encouraged realistic planning based on his availability. When needed, I stepped in to help with his foundational tasks to keep the project moving smoothly.\\nAs a result, we consistently met our milestones and delivered a functional prototype on time. My approach also encouraged my teammate to start communicating his progress earlier, which helped us work more collaboratively. This taught me how leading through action can influence others and keep a team aligned.\\n#Question#  When was the last time you asked for direct feedback from a superior? Why? #Question#\\nAt Adauris, I was developing a seasonal product promotional feature. The development went smoothly, and I felt confident that I understood the requirements. However, after completing the basic framework, I reached out to my supervisor for feedback to ensure I wasn’t missing anything.\\nIt turned out that while the framework was solid, the style of the feature didn’t align with what the cross-functional team envisioned. Thanks to the early feedback, I was able to restructure the design and adjust the styling in time. As a result, we launched the feature successfully and received positive feedback from the team.\\nThis reinforced for me the value of seeking feedback early — even when confident — to avoid costly rework and deliver results that truly meet expectations.\\n\\n #Question# Describe a time when you volunteered to expand your knowledge at work, as opposed to being directed to do so. #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked with a teammate whose module my work depended on. He had a last-minute working style, often finishing tasks near deadlines, which made it hard for me to iterate early.\\nTo keep us on track, I prioritized meeting weekly goals over maintaining strict task boundaries. I scheduled alignment meetings and, noticing he was overwhelmed, volunteered to help with foundational tasks in his module. Since his codebase was written in a framework I wasn’t familiar with, I spent extra time outside of meetings learning the framework and its APIs. This allowed me to contribute meaningfully to his module and keep our project moving.\\nAs a result, we consistently met milestones and delivered a functional prototype on time. This taught me the value of proactively expanding my knowledge to support team success.\\n\\n#Question# Tell me about a time when you had to juggle several projects at the same time. How did you organize your time? What was the result? #Question#\\nDuring my Master of Data Science at UBC, I was balancing multiple responsibilities at once: coursework, a research assistantship, and an internship at a startup. I quickly realized that trying to do everything perfectly wasn’t realistic, so I developed a system to prioritize effectively.\\nEach week, I created a task list and ranked items by importance and impact. I prioritized projects with external dependencies—like team-based assignments and internship deliverables—over individual tasks, such as certain class assignments. I then allocated my time accordingly, focusing my energy where it mattered most.\\nAs a result, I consistently delivered on my responsibilities for team projects and stakeholder-facing work, while managing individual tasks to a satisfactory level. This experience taught me the importance of prioritization and time management when juggling competing demands.\\n\\n\\n#Question# Tell me about a project that you planned. How did you organize and schedule the tasks? #Question#\\nDuring my Capstone project at BC Children’s Hospital, I worked in a team of four and took the lead on project planning. Each week, after meetings with stakeholders, new ideas and expectations would arise, and often my teammates and I had slightly different ideas about how to move forward.\\nTo keep us aligned, I always prepare for our internal team meetings by outlining clear weekly objectives and listing known challenges. This ensured our discussions stayed focused on the stakeholders’ expectations and addressed roadblocks proactively. By the end of every meeting, we were able to finalize a project plan, and I would distribute tasks based on each teammate’s strengths and availability.\\nThis approach kept the team organized and allowed us to meet all key milestones on time while accommodating stakeholder feedback throughout the project.\\n\\n#Question# Describe a time when you felt stressed or overwhelmed. How did you handle it? #Question#\\nDuring my Master of Data Science at UBC, I was juggling coursework, a research assistantship, and an internship at a startup. At one point, I felt overwhelmed trying to balance so many responsibilities and realized I couldn’t do everything perfectly.\\nTo manage the stress and productivity, I created a weekly task list and ranked items by importance and impact. I prioritized projects with external dependencies—like team-based assignments and internship deliverables—over individual tasks, such as some class assignments. I then allocated my time and energy accordingly.\\nAs a result, I consistently delivered on team projects and stakeholder-facing work while managing individual tasks to a satisfactory level. This taught me the importance of prioritization and time management when navigating competing demands.\\n\\n#Question# How do you determine what amount of time is reasonable for a task? #Question#\\nIn short, I prefer to overestimate rather than underestimate. I learned this from a research assistant experience where I was tasked with integrating an optimizer model into our pipeline. Since it was a popular open-source model, I assumed it would be quick—just a matter of reusing existing code.\\nHowever, I soon realized the model’s use case didn’t fully align with ours, and I had to dive deep into its codebase to modify it for our needs. This took much longer than expected and made me realize I hadn’t accounted for unexpected challenges.\\nSince then, my rule of thumb has been to base estimates on my past experience. And if I’m unfamiliar with a task, I would save buffer time for unknowns and avoid being overly optimistic. This approach has helped me deliver more reliable timelines and manage expectations effectively.\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc45ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "is_q = False\n",
    "question = \"\"\n",
    "answer = \"\"\n",
    "full_text_list = full_text.split()\n",
    "\n",
    "for word in full_text_list:\n",
    "    if word == \"#Question#\" and not is_q:\n",
    "        # Switch to question mode\n",
    "        if answer.strip():  # save previous answer if exists\n",
    "            answers.append(answer.strip())\n",
    "        is_q = True\n",
    "        question = \"\"\n",
    "    elif word == \"#Question#\" and is_q:\n",
    "        # Switch to answer mode\n",
    "        if question.strip():  # save previous question if exists\n",
    "            questions.append(question.strip())\n",
    "        is_q = False\n",
    "        answer = \"\"\n",
    "    else:\n",
    "        if is_q:\n",
    "            question += word + \" \"\n",
    "        else:\n",
    "            answer += word + \" \"\n",
    "\n",
    "# Save the last answer\n",
    "if answer.strip():\n",
    "    answers.append(answer.strip())\n",
    "\n",
    "assert len(questions) == len(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7721b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qa_data = [{\"question\": q, \"answer\": a} for q, a in zip(questions, answers)]\n",
    "with open(data_path + \"bq_questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce01e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"bq_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb954b",
   "metadata": {},
   "source": [
    "## Embedding Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5232aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e50f09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "dotenv_path = os.path.join(os.path.dirname(os.getcwd()), 'app/.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "embedding_model = embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf3a9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Tell me about yourself:',\n",
       "  'answer': \"My name is Daoming Liu. I'm a data science graduate with a focus on natural language processing and applied machine learning, recently graduated from UBC’s MDS in Computational Linguistics. At the Digital Lab for BC Children’s Hospital, I built a Retrieval-Augmented Generation (RAG) chatbot to help clinicians access health resources more efficiently, reducing search time and improving usability in internal workflows. At Adauris, a TTS startup, I worked on backend development, designing API endpoints that enabled real-time LLM service integration, supporting their MVP launch, and client demos.\"},\n",
       " {'question': 'Tell me about a time when you were asked to do something you had never done before. How did you react? What did you learn?',\n",
       "  'answer': 'At Adauris, I was collaborating on redesigning the backend structure for a new product feature. One of the endpoints I was assigned to update was written in TypeScript, which was a language I had never used before. At first, I was overwhelmed by the unfamiliar syntax and structure. Instead of diving in blindly, I spent the weekend learning the fundamentals of TypeScript and practicing by rewriting small components with the help of AI tools. This gave me the confidence to make meaningful updates to the endpoint. In the end, I completed the upgrade successfully, helping our team deliver the feature on time. I learned that grounding myself in the basics and quickly putting in hands-on work is a reliable strategy when facing new technical challenges.'},\n",
       " {'question': 'Recall a time when you were assigned a task outside of your job description. How did you handle the situation? What was the outcome?',\n",
       "  'answer': 'At Adauris, I was collaborating on redesigning the backend structure for a new product feature. One of the endpoints I was assigned to update was written in TypeScript, which of outside of my domain knowledge as a Python developer. When I first looked at the code, I was overwhelmed by the unfamiliar syntax and structure. My strategy of learning and tackling unfamiliar challenges was always to learn by doing. Therefore, after spending a weekend learning the fundamentals of TypeScript, I jumped right into the script and started practicing by rewriting small components with the help of AI tools. In the end, I completed the upgrade successfully, helping our team deliver the feature on time.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b10de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content = qa['question'],\n",
    "        metadata = {\n",
    "            \"answer\": qa['answer']\n",
    "        }\n",
    "    ) \n",
    "    for qa in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/daomingliu/Desktop/mini-me_llm/backend/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "../app/src/mini_me_rag/data/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2392b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "save_path = \"../app/src/mini_me_rag/data/chroma\"\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e572a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'answer': \"My name is Daoming Liu. I'm a data science graduate with a focus on natural language processing and applied machine learning, recently graduated from UBC’s MDS in Computational Linguistics. At the Digital Lab for BC Children’s Hospital, I built a Retrieval-Augmented Generation (RAG) chatbot to help clinicians access health resources more efficiently, reducing search time and improving usability in internal workflows. At Adauris, a TTS startup, I worked on backend development, designing API endpoints that enabled real-time LLM service integration, supporting their MVP launch, and client demos.\"}, page_content='Tell me about yourself:'),\n",
       "  0.19312839210033417),\n",
       " (Document(metadata={'answer': 'During my Capstone project at BC Children’s Hospital, I worked with three teammates to develop a RAG chatbot application.'}, page_content='Tell me about one of your favorite experiences working with a team and the contributions you made.'),\n",
       "  1.226933240890503),\n",
       " (Document(metadata={'answer': 'During my Capstone project at BC Children’s Hospital, I worked with three classmates to develop a RAG chatbot. There was one teammate in particular that I had to work very closely with, as my module depended on his implementation of his part. However, he had a last-minute working style and a busy schedule, and he often completed tasks close to deadlines. This conflicted with my need to iterate early. To lead by example, I made it a priority to complete my tasks ahead of schedule and share updates proactively. I also scheduled weekly alignment meetings early in each cycle and encouraged realistic planning based on his availability. When needed, I stepped in to help with his foundational tasks to keep the project moving smoothly. As a result, we consistently met our milestones and delivered a functional prototype on time. My approach also encouraged my teammate to start communicating his progress earlier, which helped us work more collaboratively. This taught me how leading through action can influence others and keep a team aligned.'}, page_content='Tell me about a time when you led by example. What did you do and how did others react?'),\n",
       "  1.2287013530731201)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"tell me about yourself\"\n",
    "docs = vector_store.similarity_search_with_score(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfe7f6",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "145ea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_PROMPT = \"\"\"\n",
    "You are a mini clone version of a person designed to answer behavioral interview questions. \n",
    "You can only answer based on the information in the provided reference question-and-answer pairs. \n",
    "If there is no reference QA pairs or none of the reference QA pairs provide enough information to fully answer the query, reply exactly with: \n",
    "\"That's a great question, and I don't seem to have an answer for that yet!\"\n",
    "\n",
    "Do NOT add any new experiences, details, or assumptions that are not directly supported by the reference content.\n",
    "\n",
    "Your goal is to respond to the query question professionally, in first person, and as if you are the person who wrote the reference QA pairs. \n",
    "\n",
    "Guidelines:\n",
    "- Be concise but complete; stay within 150 words.\n",
    "- Do not invent or guess details not present in the references.\n",
    "- If multiple reference answers partially match, synthesize them into a coherent response.\n",
    "- If references are only loosely related, acknowledge them cautiously.\n",
    "\n",
    "---\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "\n",
    "<Reference QA Pairs>\n",
    "{references}\n",
    "\n",
    "</Reference QA Pairs>\n",
    "---\n",
    "\n",
    "Your Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Agent Director Prompt (with CoT few-shot examples\n",
    "\n",
    "DIRECTOR_PROMPT = \"\"\"\n",
    "You are an intelligent agent reasoning whether any of the retrieved behavioral interview questions directly answer the user's query.\n",
    "\n",
    "Follow these steps carefully:\n",
    "1. Compare the user's query with each retrieved question.\n",
    "2. For each, explain if it is directly relevant, semantically similar, or unrelated.\n",
    "3. At the end, decide: does any retrieved question sufficiently answer the user's query?\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"reasoning\": \"Step-by-step reasoning text here.\",\n",
    "  \"final_decision\": true or false\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "Few-shot examples:\n",
    "\n",
    "User Query: \"Can you give a short introduction?\"\n",
    "Retrieved Questions:\n",
    "1. Tell me about yourself\n",
    "2. What motivates you in your career?\n",
    "3. Describe a time you had to persuade someone.\n",
    "\n",
    "Response:\n",
    "{{\n",
    "  \"reasoning\": \"1. 'Tell me about yourself' is semantically equivalent to 'give a short introduction'.\\n2. 'What motivates you...' is not a short intro.\\n3. 'Describe a time...' is unrelated.\",\n",
    "  \"final_decision\": true\n",
    "}}\n",
    "\n",
    "User Query: \"What's your favorite hobby?\"\n",
    "Retrieved Questions:\n",
    "1. Tell me about one of your favorite experiences working with a team and the contributions you made.\n",
    "2. What is your biggest strength?\n",
    "3. Describe a time when you felt stressed and how you handled it.\n",
    "\n",
    "Response:\n",
    "{{\n",
    "  \"reasoning\": \"1. 'Favorite experiences working with a team' relates to professional experiences, not hobbies.\\n2. 'What is your biggest strength?' is unrelated to hobbies.\\n3. 'Describe a time...' is unrelated.\",\n",
    "  \"final_decision\": false\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "Now do the same for:\n",
    "\n",
    "User Query: \"{query}\"\n",
    "Retrieved Questions:\n",
    "{retrieved_text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "REWRITE_PROMPT = \"\"\"\n",
    "You are an assistant helping rewrite user queries into typical behavioral interview questions stored in a knowledge base.\n",
    "\n",
    "Your job:\n",
    "- Rewrite the user query into a clear, standard behavioral interview question.\n",
    "- Do not add new details or assumptions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Few-shot examples:\n",
    "\n",
    "User Query: \"Can you give me a brief introduction?\"\n",
    "Rewritten: Tell me about yourself\n",
    "\n",
    "User Query: \"Have you ever worked with someone difficult?\"\n",
    "Rewritten: Give an example of when you had to work with someone who was difficult to get along with. How did you handle interactions with that person?\n",
    "\n",
    "User Query: \"What’s the biggest change you’ve faced?\"\n",
    "Rewritten: Tell me about the biggest change you’ve had to deal with. How did you adapt to that change?\n",
    "\n",
    "User Query: \"When did you last ask your manager for feedback?\"\n",
    "Rewritten: When was the last time you asked for direct feedback from a superior? Why?\n",
    "\n",
    "User Query: \"Describe a project you planned from start to finish.\"\n",
    "Rewritten: Tell me about a project that you planned. How did you organize and schedule the tasks?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Now rewrite this:\n",
    "\n",
    "User Query: \"{original_query}\"\n",
    "Rewritten:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7959474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "class MiniMeAnswer(BaseModel):\n",
    "    \"\"\"Answer to user query.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"The first-person answer to user query.\")\n",
    "    reasoning: str = Field(description=\"Reasoning on which story provided is related to this query.\")\n",
    "\n",
    "\n",
    "class MiniMe:\n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        llm = ChatOpenAI()\n",
    "        self.llm_client = llm.with_structured_output(MiniMeAnswer)\n",
    "        self.generation_prompt = GENERATION_PROMPT\n",
    "        \n",
    "    def retrieve(self, query, verbose = False, max_dist = 2):\n",
    "        retireved_docs = vector_store.similarity_search_with_score(query, k=3)\n",
    "        \n",
    "        if verbose:\n",
    "            return [doc for doc in retireved_docs if doc[1] <= max_dist]\n",
    "        \n",
    "        full_retrieved_text = \"\"\n",
    "        for i, doc in enumerate(retireved_docs):\n",
    "            if doc[1] > max_dist:\n",
    "                continue\n",
    "            full_retrieved_text += f\"Reference Question #{i}:\"\n",
    "            full_retrieved_text += doc[0].page_content\n",
    "            full_retrieved_text += \"\\n\"\n",
    "            full_retrieved_text += f\"Reference Answer #{i}:\"\n",
    "            full_retrieved_text += doc[0].metadata['answer']\n",
    "            full_retrieved_text += \"\\n\\n\"\n",
    "\n",
    "        return full_retrieved_text\n",
    "    \n",
    "    def generate(self, query, verbose = False):\n",
    "        references = self.retrieve(query)\n",
    "        \n",
    "        prompt_text = self.prompt.format(query=query, references=references)\n",
    "        output = self.llm_client.invoke(prompt_text) \n",
    "        \n",
    "        if verbose:\n",
    "            return (self.retrieve(query, verbose), prompt_text, output)\n",
    "        \n",
    "        answer = output.answer\n",
    "        return answer\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3981ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "\n",
    "# Director structured output model\n",
    "class DirectorDecision(BaseModel):\n",
    "    \"\"\"Decision from the agent director.\"\"\"\n",
    "    reasoning: str = Field(description=\"Step-by-step reasoning on retrieved documents relevance.\")\n",
    "    final_decision: bool = Field(description=\"True if any retrieved doc answers the query, False otherwise.\")\n",
    "\n",
    "# RAG final answer model (with CoT reasoning)\n",
    "class MiniMeAnswer(BaseModel):\n",
    "    \"\"\"Answer to user query.\"\"\"\n",
    "    answer: str = Field(description=\"The first-person answer to user query.\")\n",
    "    reasoning: str = Field(description=\"Reasoning on which story provided is related to this query.\")\n",
    "\n",
    "# Full Workflow\n",
    "class MiniMe:\n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOpenAI()\n",
    "        self.director_client = self.llm.with_structured_output(DirectorDecision)\n",
    "        self.generator_client = self.llm.with_structured_output(MiniMeAnswer)\n",
    "        self.generation_prompt = GENERATION_PROMPT\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        \"\"\"Retrieve top-k documents from vector store\"\"\"\n",
    "        return self.vector_store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    def agent_director(self, query, retrieved_docs, verbose=False):\n",
    "        \"\"\"Agent director reasons if retrieved docs answer the query.\"\"\"\n",
    "        retrieved_text = \"\\n\".join(\n",
    "            [f\"{i+1}. {doc[0].page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "        )\n",
    "        prompt = DIRECTOR_PROMPT.format(query=query, retrieved_text=retrieved_text)\n",
    "        decision = self.director_client.invoke(prompt)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n=== Agent Director Reasoning ===\")\n",
    "            print(decision.reasoning)\n",
    "            print(\"Final Decision:\", \"YES\" if decision.final_decision else \"NO\")\n",
    "\n",
    "        return decision\n",
    "\n",
    "    def rewrite_and_retrieve(self, original_query, k=3, verbose=False):\n",
    "        \"\"\"Rewrite the query and retrieve new documents\"\"\"\n",
    "        prompt = REWRITE_PROMPT.format(original_query=original_query)\n",
    "        rewritten_query = self.llm.invoke(prompt).content.strip()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Query Rewriting ===\")\n",
    "            print(f\"Original Query: {original_query}\")\n",
    "            print(f\"Rewritten Query: {rewritten_query}\")\n",
    "\n",
    "        if rewritten_query.upper() == \"UNRELATED\":\n",
    "            return rewritten_query, []\n",
    "\n",
    "        new_docs = self.retrieve(rewritten_query, k)\n",
    "        return rewritten_query, new_docs\n",
    "\n",
    "    def generate_final_answer(self, query, retrieved_docs, start_time, verbose):\n",
    "        \"\"\"Compose final answer using generation LLM\"\"\"\n",
    "        references = \"\"\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            references += f\"Reference Question #{i+1}:\\n{doc[0].page_content}\\n\"\n",
    "            references += f\"Reference Answer #{i+1}:\\n{doc[0].metadata.get('answer', '')}\\n\\n\"\n",
    "\n",
    "        prompt_text = self.generation_prompt.format(query=query, references=references)\n",
    "        result = self.generator_client.invoke(prompt_text)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(\"\\n=== Final Generation CoT Reasoning ===\")\n",
    "        print(result.reasoning)\n",
    "        print(f\"Total Time: {total_time:.2f}s\")\n",
    "        return {\n",
    "            \"references\": references,\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"output\": result,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "\n",
    "    def generate(self, query, max_retries=2, verbose=False):\n",
    "        \"\"\"Main workflow: retrieve → reason → rewrite+retrieve (max 2) → generate\"\"\"\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"\\n=== Agentic Workflow Started ===\")\n",
    "\n",
    "        # Step 1: Initial retrieval\n",
    "        retrieved_docs = self.retrieve(query)\n",
    "        if verbose:\n",
    "            print(f\"Initial retrieval done. Retrieved {len(retrieved_docs)} docs.\")\n",
    "\n",
    "        # Step 2: Director reasoning\n",
    "        decision = self.agent_director(query, retrieved_docs, verbose)\n",
    "        if decision.final_decision:\n",
    "            if verbose:\n",
    "                print(\"Director: Initial retrieval is sufficient.\")\n",
    "            return self.generate_final_answer(query, retrieved_docs, start_time, verbose)\n",
    "\n",
    "        # Retry Loop: Up to max_retries\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            if verbose:\n",
    "                print(f\"\\nDirector: Retrieval insufficient. Attempting rewrite+retrieve (Retry #{attempt})...\")\n",
    "            rewritten_query, retrieved_docs = self.rewrite_and_retrieve(query, verbose=verbose)\n",
    "            if rewritten_query.upper() == \"UNRELATED\":\n",
    "                if verbose:\n",
    "                    print(\"Rewrite tool determined the query is unrelated.\")\n",
    "                return self.generate_final_answer(query, [], start_time, verbose)\n",
    "            decision = self.agent_director(query, retrieved_docs, verbose)\n",
    "            if decision.final_decision:\n",
    "                if verbose:\n",
    "                    print(f\"Director: Retrieval after rewrite #{attempt} is sufficient.\")\n",
    "                return self.generate_final_answer(query, retrieved_docs, start_time, verbose)\n",
    "\n",
    "        # If still insufficient after retries\n",
    "        if verbose:\n",
    "            print(\"Director: All retries exhausted. No sufficient docs found.\")\n",
    "        return self.generate_final_answer(query, [], start_time, verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21b8bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daomingliu/miniforge3/envs/mini-me/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1857: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "minime = MiniMe(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b069b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Generation CoT Reasoning ===\n",
      "\n",
      "Total Time: 9.39s\n"
     ]
    }
   ],
   "source": [
    "query = \"What is your working style like?\"\n",
    "\n",
    "#minime.retrieve(query, True)\n",
    "\n",
    "output = minime.generate(query, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4644dad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'references': '',\n",
       " 'prompt_text': '\\nYou are a mini clone version of a person designed to answer behavioral interview questions. \\nYou can only answer based on the information in the provided reference question-and-answer pairs. \\nIf there is no reference QA pairs or none of the reference QA pairs provide enough information to fully answer the query, reply exactly with: \\n\"That\\'s a great question, and I don\\'t seem to have an answer for that yet!\"\\n\\nDo NOT add any new experiences, details, or assumptions that are not directly supported by the reference content.\\n\\nYour goal is to respond to the query question professionally, in first person, and as if you are the person who wrote the reference QA pairs. \\n\\nGuidelines:\\n- Be concise but complete; stay within 150 words.\\n- Do not invent or guess details not present in the references.\\n- If multiple reference answers partially match, synthesize them into a coherent response.\\n- If references are only loosely related, acknowledge them cautiously.\\n\\n---\\n\\n<Query>\\nWhat is your working style like?\\n</Query>\\n\\n<Reference QA Pairs>\\n\\n\\n</Reference QA Pairs>\\n---\\n\\nYour Response:\\n\\n',\n",
       " 'output': MiniMeAnswer(answer=\"That's a great question, and I don't seem to have an answer for that yet!\", reasoning='No reference QA pairs provided on the working style.'),\n",
       " 'total_time': 7.634119033813477}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "170bd849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great question, and I don't seem to have an answer for that yet!\n"
     ]
    }
   ],
   "source": [
    "print(output['output'].answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
